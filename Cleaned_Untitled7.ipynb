{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d1c09da2-96eb-4e3c-8a00-ef92462f96b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.impute import SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "971b2251-973a-4352-9d3b-76c1f61e7e19",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "unterminated string literal (detected at line 15) (1215458159.py, line 15)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[2], line 15\u001b[1;36m\u001b[0m\n\u001b[1;33m    print(\"\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m unterminated string literal (detected at line 15)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Load the dataset with error handling\n",
    "file_path = 'LG_Customer_Data_30.csv'  # Replace with the actual file path\n",
    "try:\n",
    "    customer_data = pd.read_csv(file_path)\n",
    "    print(\"Dataset loaded successfully!\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: The file '{file_path}' was not found. Please provide a valid file path.\")\n",
    "    raise\n",
    "\n",
    "# Display the first few rows to understand the data structure\n",
    "print(\"First 5 rows of the dataset:\")\n",
    "print(customer_data.head())\n",
    "\n",
    "# Display dataset info\n",
    "print(\"\n",
    "Dataset Info:\")\n",
    "print(customer_data.info())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff10cec-2df4-40ad-aa1a-a40b77ec5b26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for missing values\n",
    "print(\"\\nMissing Values:\")\n",
    "print(customer_data.isnull().sum())\n",
    "\n",
    "# Impute missing values for numerical columns using the median\n",
    "numerical_columns = customer_data.select_dtypes(include=['float64', 'int64']).columns\n",
    "imputer = SimpleImputer(strategy='median')\n",
    "customer_data[numerical_columns] = imputer.fit_transform(customer_data[numerical_columns])\n",
    "\n",
    "print(\"\\nMissing values handled. Dataset updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc87e9c-3ac1-4d70-95b7-4df1db5b812d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Identify categorical columns\n",
    "categorical_columns = customer_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "# Apply encoding for categorical columns\n",
    "for column in categorical_columns:\n",
    "    if customer_data[column].nunique() <= 10:  # Use label encoding for low-cardinality columns\n",
    "        customer_data[column] = customer_data[column].astype('category').cat.codes\n",
    "    else:  # Use one-hot encoding for high-cardinality columns\n",
    "        customer_data = pd.get_dummies(customer_data, columns=[column], drop_first=True)\n",
    "\n",
    "print(\"Categorical columns encoded.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ab3950a-0fef-4892-8609-56aa003d0925",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize numerical data using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "customer_data[numerical_columns] = scaler.fit_transform(customer_data[numerical_columns])\n",
    "\n",
    "print(\"\\nNumerical data normalized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dfbc49-0ce0-4d80-9927-29ad85cc1a21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the Within-Cluster Sum of Squares (WCSS) for 1-10 clusters\n",
    "wcss = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(customer_data[numerical_columns])\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the Elbow Method\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), wcss, marker='o', linestyle='-', color='b')\n",
    "plt.title('Elbow Method for Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('WCSS (Within-cluster Sum of Squares)')\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nCheck the plot to determine the optimal number of clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c65ab4-87ad-44cd-9de4-edd2e6dc70d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the number of clusters (based on the Elbow Method plot)\n",
    "optimal_clusters = 4  # Adjust based on the Elbow Method\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "customer_data['Cluster'] = kmeans.fit_predict(customer_data[numerical_columns])\n",
    "\n",
    "print(\"\\nClusters assigned to each data point.\")\n",
    "print(customer_data[['Cluster']].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5862201-dcbb-458b-b644-c87ced5ce18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if columns 'Income' and 'Average_Spend' exist for plotting\n",
    "if 'Income' in customer_data.columns and 'Average_Spend' in customer_data.columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.scatterplot(\n",
    "        x=customer_data['Income'], \n",
    "        y=customer_data['Average_Spend'], \n",
    "        hue=customer_data['Cluster'], \n",
    "        palette='viridis', \n",
    "        s=100, \n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title('Customer Segments based on Income and Average Spend')\n",
    "    plt.xlabel('Income')\n",
    "    plt.ylabel('Average Spend')\n",
    "    plt.legend(title=\"Cluster\")\n",
    "    plt.show()\n",
    "else:\n",
    "    print(\"\\nColumns 'Income' and 'Average_Spend' not found for plotting.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f57133d-ba45-43b2-99dc-405376dea02f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze cluster centroids\n",
    "cluster_centers = pd.DataFrame(kmeans.cluster_centers_, columns=numerical_columns)\n",
    "print(\"\\nCluster Centers:\")\n",
    "print(cluster_centers)\n",
    "\n",
    "# Add cluster summary to the processed dataset\n",
    "cluster_summary = customer_data.groupby('Cluster').mean()\n",
    "print(\"\\nCluster Summary Statistics:\")\n",
    "print(cluster_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d53d4f8-8792-4b4e-aef7-f3976a7cf0d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Perform ANOVA for each feature to check for differences across clusters\n",
    "anova_results = {}\n",
    "for column in numerical_columns:\n",
    "    groups = [customer_data[customer_data['Cluster'] == cluster][column] for cluster in range(optimal_clusters)]\n",
    "    f_stat, p_val = f_oneway(*groups)\n",
    "    anova_results[column] = {'F-Statistic': f_stat, 'P-Value': p_val}\n",
    "\n",
    "# Display features with significant differences\n",
    "significant_features = {k: v for k, v in anova_results.items() if v['P-Value'] < 0.05}\n",
    "print(\"\\nSignificant Features Across Clusters:\")\n",
    "for feature, stats in significant_features.items():\n",
    "    print(f\"{feature}: F-Statistic = {stats['F-Statistic']:.2f}, P-Value = {stats['P-Value']:.5f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac5089b-0f17-401c-a54e-9e28b278882e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bar plot for cluster sizes\n",
    "plt.figure(figsize=(8, 5))\n",
    "customer_data['Cluster'].value_counts().plot(kind='bar', color='skyblue', alpha=0.8)\n",
    "plt.title('Cluster Sizes')\n",
    "plt.xlabel('Cluster')\n",
    "plt.ylabel('Number of Customers')\n",
    "plt.show()\n",
    "\n",
    "# Box plot for numerical features grouped by cluster\n",
    "for column in numerical_columns:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    sns.boxplot(x='Cluster', y=column, data=customer_data, palette='viridis')\n",
    "    plt.title(f'Feature Distribution by Cluster: {column}')\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel(column)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70e519fc-640a-434f-80fc-d5f5425cc0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install aws lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "105cdaf9-76f6-48b9-b9a6-3a27b36ec382",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.datasets import make_blobs\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set parameters for synthetic dataset\n",
    "n_samples = 300  # Total number of data points\n",
    "n_features = 2   # Number of numerical features\n",
    "n_clusters = 4   # Number of clusters\n",
    "\n",
    "# Generate synthetic dataset with clusters\n",
    "data, labels = make_blobs(n_samples=n_samples, n_features=n_features, centers=n_clusters, random_state=42)\n",
    "\n",
    "# Create a DataFrame from the generated data\n",
    "columns = ['Income', 'Average_Spend']  # Feature names\n",
    "customer_data = pd.DataFrame(data, columns=columns)\n",
    "\n",
    "# Add categorical features\n",
    "customer_data['Age_Group'] = np.random.choice(['Young', 'Adult', 'Senior'], size=n_samples)\n",
    "customer_data['Loyalty_Score'] = np.random.randint(1, 11, size=n_samples)  # 1 to 10 scale\n",
    "customer_data['Preferred_Channel'] = np.random.choice(['Online', 'In-Store', 'Mobile'], size=n_samples)\n",
    "\n",
    "# Add cluster labels (for verification or testing purposes)\n",
    "customer_data['Cluster'] = labels\n",
    "\n",
    "# Save to CSV (optional)\n",
    "customer_data.to_csv('synthetic_customer_data.csv', index=False)\n",
    "\n",
    "# Display sample rows\n",
    "print(customer_data.head())\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x='Income', y='Average_Spend', hue='Cluster', data=customer_data, palette='viridis', s=100, alpha=0.8)\n",
    "plt.title(\"Synthetic Customer Data Clusters\")\n",
    "plt.xlabel(\"Income\")\n",
    "plt.ylabel(\"Average Spend\")\n",
    "plt.legend(title=\"Cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ff15df-7ce7-442b-9d46-5c575d44bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Define the features for clustering (normalized values of income and spend)\n",
    "X = customer_data[['Income', 'Average_Spend']]\n",
    "\n",
    "# Determine the optimal number of clusters using the elbow method\n",
    "inertia = []\n",
    "for i in range(1, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    inertia.append(kmeans.inertia_)\n",
    "\n",
    "# Plot the elbow graph\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot(range(1, 11), inertia, marker='o')\n",
    "plt.title('Elbow Method for Optimal Clusters')\n",
    "plt.xlabel('Number of Clusters')\n",
    "plt.ylabel('Inertia')\n",
    "plt.show()\n",
    "\n",
    "# Applying KMeans with the optimal number of clusters (let's assume 3 clusters based on elbow method)\n",
    "optimal_clusters = 3  # Adjust based on your elbow method outcome\n",
    "kmeans = KMeans(n_clusters=optimal_clusters, random_state=42)\n",
    "customer_data['Cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Show sample of customer data with cluster assignment\n",
    "print(customer_data[['Cluster', 'Income', 'Average_Spend']].head())\n",
    "\n",
    "# Visualize the clusters\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=customer_data['Income'], y=customer_data['Average_Spend'], hue=customer_data['Cluster'], palette='viridis', s=100, alpha=0.7)\n",
    "plt.title('Customer Segments based on Income and Average Spend')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Average Spend')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cafcfa69-c9ba-4c73-b027-e24af1128896",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example demand elasticity calculation\n",
    "# Assuming columns 'Price' and 'Quantity' are available in the dataset\n",
    "def calculate_elasticity(price_initial, price_final, quantity_initial, quantity_final):\n",
    "    price_change = (price_final - price_initial) / price_initial\n",
    "    quantity_change = (quantity_final - quantity_initial) / quantity_initial\n",
    "    return quantity_change / price_change\n",
    "\n",
    "# Sample data (replace with actual data)\n",
    "customer_data['Price_Initial'] = customer_data['Average_Spend']  # Assuming Average_Spend acts as price\n",
    "customer_data['Quantity_Initial'] = customer_data['Average_Spend'] * 10  # Placeholder for quantity data\n",
    "customer_data['Price_Final'] = customer_data['Price_Initial'] * 1.1  # Example: price increase of 10%\n",
    "customer_data['Quantity_Final'] = customer_data['Quantity_Initial'] * 0.9  # Assuming demand drops with price increase\n",
    "\n",
    "# Calculate elasticity for each customer segment\n",
    "customer_data['Elasticity'] = customer_data.apply(lambda row: calculate_elasticity(\n",
    "    row['Price_Initial'], row['Price_Final'], row['Quantity_Initial'], row['Quantity_Final']), axis=1)\n",
    "\n",
    "# Display the elasticity values\n",
    "print(customer_data[['Cluster', 'Elasticity']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1a4bc4a-2eec-46dc-9f36-b7190b26f407",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Price optimization based on elasticity\n",
    "def optimize_price(elasticity, price):\n",
    "    if elasticity > 0:  # Elastic product: decrease price\n",
    "        return price * 0.9\n",
    "    elif elasticity < 0:  # Inelastic product: increase price\n",
    "        return price * 1.1\n",
    "    else:  # Unchanged demand: no price change\n",
    "        return price\n",
    "\n",
    "# Apply price optimization\n",
    "customer_data['Optimized_Price'] = customer_data.apply(lambda row: optimize_price(row['Elasticity'], row['Price_Initial']), axis=1)\n",
    "\n",
    "# Show the updated prices\n",
    "print(customer_data[['Cluster', 'Price_Initial', 'Optimized_Price']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3c2686e0-355c-494c-ade6-66b224024780",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'customer_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[3], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Visualize the price optimization\u001b[39;00m\n\u001b[0;32m      2\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 3\u001b[0m sns\u001b[38;5;241m.\u001b[39mscatterplot(x\u001b[38;5;241m=\u001b[39mcustomer_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncome\u001b[39m\u001b[38;5;124m'\u001b[39m], y\u001b[38;5;241m=\u001b[39mcustomer_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAverage_Spend\u001b[39m\u001b[38;5;124m'\u001b[39m], hue\u001b[38;5;241m=\u001b[39mcustomer_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimized_Price\u001b[39m\u001b[38;5;124m'\u001b[39m], palette\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcoolwarm\u001b[39m\u001b[38;5;124m'\u001b[39m, s\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.7\u001b[39m)\n\u001b[0;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mtitle(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOptimized Pricing Based on Demand Elasticity\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mIncome\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'customer_data' is not defined"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x600 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the price optimization\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.scatterplot(x=customer_data['Income'], y=customer_data['Average_Spend'], hue=customer_data['Optimized_Price'], palette='coolwarm', s=100, alpha=0.7)\n",
    "plt.title('Optimized Pricing Based on Demand Elasticity')\n",
    "plt.xlabel('Income')\n",
    "plt.ylabel('Original Price (Average Spend)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03012d85-ae33-4fac-885c-754b471505c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "# Save the trained model\n",
    "model_path = 'kmeans_clustering_model.pkl'\n",
    "joblib.dump(\"kmeans_clustering_model.pklh\", \"LG_Customer_Data_30.csv\")\n",
    "print(f\"\\nKMeans model saved to {\"kmeans_clustering_model.pkl\"}\")\n",
    "\n",
    "# Save the processed dataset for deployment\n",
    "processed_data_path = 'processed_customer_data.csv'\n",
    "customer_data.to_csv(processed_data_path, index=False)\n",
    "print(f\"\\nProcessed data saved to {\"kmeans_clustering_model.pkl\"}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f601df11-fca7-4239-8821-b27cdbdb101a",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51b3b8a-c366-4199-9bc2-bd64ea5bce7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "model_path = os.path.join(os.getcwd(), 'kmeans_model.pkl')\n",
    "scaler_path = os.path.join(os.getcwd(), 'scaler.pkl')\n",
    "\n",
    "with open(model_path, 'rb') as model_file:\n",
    "    kmeans_model = pickle.load(model_file)\n",
    "with open(scaler_path, 'rb') as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb402add-ed75-4c6f-8da3-8c4b9bd29125",
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install flask_cors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c533aea5-4311-46f5-a9b5-9245986e3f46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from flask import Flask, request, jsonify\n",
    "from flask_cors import CORS\n",
    "\n",
    "# Initialize Flask app\n",
    "app = Flask(__name__)\n",
    "CORS(app)  # Enable CORS for cross-origin requests\n",
    "\n",
    "# Load the model and scaler\n",
    "with open('kmeans_model.pkl', 'rb') as model_file:\n",
    "    kmeans_model = pickle.load(model_file)\n",
    "with open('scaler.pkl', 'rb') as scaler_file:\n",
    "    scaler = pickle.load(scaler_file)\n",
    "\n",
    "# Define a route for predictions\n",
    "@app.route('/predict', methods=['POST'])\n",
    "def predict():\n",
    "    try:\n",
    "        # Log the incoming request\n",
    "        print(\"Received data:\", request.json)\n",
    "\n",
    "        # Get JSON data from the request\n",
    "        data = request.get_json()\n",
    "\n",
    "        # Extract and validate inputs\n",
    "        income = float(data.get('Income', 0))  # Default to 0 if key is missing\n",
    "        avg_spend = float(data.get('Average_Spend', 0))  # Default to 0 if key is missing\n",
    "\n",
    "        # Log extracted data\n",
    "        print(f\"Income: {income}, Average Spend: {avg_spend}\")\n",
    "\n",
    "        # Preprocess input data\n",
    "        input_features = np.array([[income, avg_spend]])\n",
    "        input_scaled = scaler.transform(input_features)\n",
    "\n",
    "        # Make prediction\n",
    "        cluster = kmeans_model.predict(input_scaled)[0]\n",
    "\n",
    "        # Return the prediction\n",
    "        return jsonify({'cluster': int(cluster)})\n",
    "\n",
    "    except Exception as e:\n",
    "        # Log and return the error\n",
    "        print(f\"Error: {str(e)}\")\n",
    "        return jsonify({'error': str(e)}), 400\n",
    "\n",
    "# Run the app for local testing\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c312d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Save the preprocessed dataset for further use or deployment\n",
    "output_path = 'cleaned_customer_data.csv'\n",
    "customer_data.to_csv(output_path, index=False)\n",
    "print(f\"Cleaned dataset saved to {output_path}.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea3a80e9-84fe-43e6-8c41-d5689f900935",
   "metadata": {},
   "outputs": [],
   "source": [
    "jupyter nbconvert --to script Cleaned_Untitled7.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d786a02-7b72-4359-9673-c7888afa6cdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[WinError 2] The system cannot find the file specified: '/cleaned_customer_data.csv'\n",
      "C:\\Users\\USER\\Documents\n"
     ]
    }
   ],
   "source": [
    "cd /cleaned_customer_data.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af1fbb6b-b12b-42de-a46a-8449074628bf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
